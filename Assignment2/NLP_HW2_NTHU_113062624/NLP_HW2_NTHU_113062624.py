# -*- coding: utf-8 -*-
"""NLP_HW2_NTHU_113062624.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X7jKB3AlA9CqlKz5DDc364v_5uY2mJrR
"""

import pandas as pd

df_train = pd.read_csv('arithmetic_train.csv')
df_eval = pd.read_csv('arithmetic_eval.csv')

print(df_train.head())
print(df_eval.head())

# transform the input data to string
df_train['tgt'] = df_train['tgt'].apply(lambda x: str(x))
df_train['src'] = df_train['src'].add(df_train['tgt'])
df_train['len'] = df_train['src'].apply(lambda x: len(x))

df_eval['tgt'] = df_eval['tgt'].apply(lambda x: str(x))
df_eval['src'] = df_eval['src'].add(df_eval['tgt'])
df_eval['len'] = df_eval['src'].apply(lambda x: len(x))

print(df_train.head())
print(df_eval.head())

"""# **TODO1: Build your dictionary here**"""

# Define character set
chars = ['<pad>', '<eos>'] + [str(i) for i in range(10)] + ['+', '-', '*', '=', '(', ')']

# Create char_to_id dictionary
char_to_id = {char: idx for idx, char in enumerate(chars)}

# Create id_to_char dictionary
id_to_char = {idx: char for char, idx in char_to_id.items()}

# Print results for checking
print("char_to_id:", char_to_id)
print("id_to_char:", id_to_char)

"""# **TODO2: Data preprocessing**"""

# The code is generated by ChatGPT and then rewritten.
# Function to convert equation into char_id_list
def encode_equation(equation, char_to_id):
    # Convert each character in the equation into its corresponding id
    char_id_list = [char_to_id[char] for char in equation]
    # Add <eos> token (represented by 1)
    char_id_list.append(char_to_id['<eos>'])
    return char_id_list

# Function to create label_id_list (only digits after '=' are encoded, rest is <pad>)
def create_label_id_list(equation, char_to_id):
    # Find the position of '='
    equal_pos = equation.index('=')
    label_id_list = []

    # Before '=' everything is <pad>
    for i in range(equal_pos + 1):
        label_id_list.append(char_to_id['<pad>'])

    # After '=' convert digits and add <eos>
    for char in equation[equal_pos + 1:]:
        label_id_list.append(char_to_id[char])
    # Add <eos> token (represented by 1)
    label_id_list.append(char_to_id['<eos>'])

    return label_id_list

# Apply the functions to df_train
df_train['char_id_list'] = df_train['src'].apply(lambda x: encode_equation(x, char_to_id))
df_train['label_id_list'] = df_train['src'].apply(lambda x: create_label_id_list(x, char_to_id))

# Check the results
print(df_train[['src', 'char_id_list', 'label_id_list']].head())

# Apply the functions to df_eval (same as df_train)
df_eval['char_id_list'] = df_eval['src'].apply(lambda x: encode_equation(x, char_to_id))
df_eval['label_id_list'] = df_eval['src'].apply(lambda x: create_label_id_list(x, char_to_id))

# Check the results for df_eval
print(df_eval[['src', 'char_id_list', 'label_id_list']].head())

"""# **TODO3: Data Batching**"""

# The code is generated by ChatGPT and then rewritten.
import torch

class Dataset(torch.utils.data.Dataset):
    def __init__(self, df):
        # Store the dataframe which contains 'char_id_list' and 'label_id_list'
        self.df = df

    def __len__(self):
        # Return the total number of data points in the dataframe
        return len(self.df)

    def __getitem__(self, index):
        # Extract the input data (features) and the ground truth (labels)
        char_id_list = self.df['char_id_list'].iloc[index]
        label_id_list = self.df['label_id_list'].iloc[index]

        # Find the index of '=' to separate x and y
        equal_x_index = char_id_list.index(char_to_id['='])  # Find the position of '='

        # x: the entire equation up to '=' (including '=' itself)
        x = char_id_list[:equal_x_index + 1]  # Keep the part before '=', including '='

        # y: the result part after '=' (excluding '=' itself)
        y = label_id_list[equal_x_index + 1:]  # y is the part after '=', excluding '='

        return x, y


# Initialize the dataset using df_train
train_dataset = Dataset(df_train)

# For evaluation data
eval_dataset = Dataset(df_eval)

from torch.utils.data import DataLoader

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def collate_fn(batch):
    # Separately extract inputs and labels
    inputs, targets = zip(*batch)

    # Find the length of the longest sequence in the batch
    max_input_len = max(len(seq) for seq in inputs)
    max_target_len = max(len(seq) for seq in targets)

    # Pad the sequences
    padded_inputs = [seq + [char_to_id['<pad>']] * (max_input_len - len(seq)) for seq in inputs]
    padded_targets = [seq + [char_to_id['<pad>']] * (max_target_len - len(seq)) for seq in targets]

    # Convert to tensors
    input_tensor = torch.tensor(padded_inputs, dtype=torch.long)
    target_tensor = torch.tensor(padded_targets, dtype=torch.long)

    return input_tensor.to(device), target_tensor.to(device)


# Initialize DataLoader for training dataset
train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=collate_fn)

# Initialize DataLoader for evaluation dataset
eval_loader = DataLoader(eval_dataset, batch_size=128, shuffle=False, collate_fn=collate_fn)

# The code is generated by ChatGPT and then rewritten.
import torch

class CharRNN(torch.nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim):
        super(CharRNN, self).__init__()

        # Define the embedding layer
        self.embedding = torch.nn.Embedding(num_embeddings=vocab_size,
                                            embedding_dim=embed_dim,
                                            padding_idx=char_to_id['<pad>'])

        # Change to two LSTM layers
        self.rnn_layer1 = torch.nn.LSTM(input_size=embed_dim,
                                        hidden_size=hidden_dim,
                                        batch_first=True)

        self.rnn_layer2 = torch.nn.LSTM(input_size=hidden_dim,
                                        hidden_size=hidden_dim,
                                        batch_first=True)

        # Define the linear layers
        self.linear = torch.nn.Sequential(
            torch.nn.Linear(in_features=hidden_dim, out_features=hidden_dim),
            torch.nn.ReLU(),
            torch.nn.Linear(in_features=hidden_dim, out_features=vocab_size)
        )

    def init_hidden(self, batch_size):
        """Initialize the hidden states for both LSTM layers."""
        hidden_state1 = (torch.zeros(1, batch_size, self.rnn_layer1.hidden_size).to(device),
                         torch.zeros(1, batch_size, self.rnn_layer1.hidden_size).to(device))
        hidden_state2 = (torch.zeros(1, batch_size, self.rnn_layer2.hidden_size).to(device),
                         torch.zeros(1, batch_size, self.rnn_layer2.hidden_size).to(device))
        return hidden_state1, hidden_state2

    def forward(self, x, hidden_state1, hidden_state2):
        """Perform the forward pass through the two LSTM layers."""
        x = self.embedding(x)

        # Pass through the first LSTM layer
        out1, hidden_state1 = self.rnn_layer1(x, hidden_state1)

        # Pass through the second LSTM layer
        out2, hidden_state2 = self.rnn_layer2(out1, hidden_state2)

        # Pass through the linear layers
        out = self.linear(out2)
        return out, hidden_state1, hidden_state2

    def predict_next_char(self, y):
        """Get the predicted next character from the output probabilities."""
        return torch.argmax(y, dim=1)

    def generator(self, start_char, max_len=200):
        """Generate characters using the trained model."""
        id_list = [char_to_id[char] for char in start_char]
        id_list = id_list + [char_to_id['<pad>']] * (11 - len(id_list)) if len(id_list) < 11 else id_list
        hidden_state1, hidden_state2 = self.init_hidden(batch_size=1)

        output_ids = []

        for t in range(max_len):
            input_tensor = torch.tensor([id_list], dtype=torch.long).to(device)
            output, hidden_state1, hidden_state2 = self.forward(input_tensor, hidden_state1, hidden_state2)
            y = output[:, -1, :]
            next_char = self.predict_next_char(y).item()
            id_list.append(next_char)
            if next_char == char_to_id['<eos>']:
                break
            output_ids.append(next_char)
        output_char = ''.join([id_to_char[id] for id in output_ids])
        return output_char

"""# **TODO5: Training**"""

# The code is generated by ChatGPT and then rewritten.
import torch
import torch.optim as optim
import torch.nn.functional as F

# Assume we already have these parameters
vocab_size = len(char_to_id)  # Size of the vocabulary
embed_dim = 128  # Dimension of the embedding layer
hidden_dim = 256  # Dimension of the LSTM hidden layer
num_epochs = 20  # Number of training epochs
learning_rate = 0.001  # Learning rate

# Initialize the model
model = CharRNN(vocab_size, embed_dim, hidden_dim).to(device)
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Training process
for epoch in range(num_epochs):
    model.train()  # Set the model to training mode
    total_loss = 0

    for x, y in train_loader:  # Iterate over each batch
        optimizer.zero_grad()  # Clear gradients

        # Move x and y to the correct device
        x, y = x.to(device), y.to(device)

        # Initialize the hidden states for both LSTM layers
        hidden_state1, hidden_state2 = model.init_hidden(batch_size=x.size(0))

        # Teacher Forcing
        max_len = y.size(1)  # The length of y is the length of the answer
        loss = 0

        # First, pass x through the model for forward propagation
        output, hidden_state1, hidden_state2 = model(x, hidden_state1, hidden_state2)

        for t in range(max_len):
            if t == 0:
                # The initial input is x, which is '23+55='
                input_seq = x  # x already contains the equation part including the equal sign
            else:
                # Concatenate the previous output y to the input at each step
                input_seq = torch.cat([x, y[:, :t]], dim=1)  # Concatenate x and the first t characters of y

            # Forward propagate with the new input using the two LSTM layers
            output, hidden_state1, hidden_state2 = model(input_seq, hidden_state1, hidden_state2)

            # Take the output from the last step to predict the next character
            prev_char = output[:, -1, :]

            # Compute the loss
            loss += F.cross_entropy(prev_char, y[:, t], ignore_index=char_to_id['<pad>'])

        total_loss += loss.item() / max_len

        # Backward pass
        loss.backward()
        optimizer.step()  # Update model parameters

    # Print the average loss at the end of each epoch
    avg_loss = total_loss / len(train_loader)
    print(f"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}")

# Save the model
torch.save(model.state_dict(), 'char_rnn_model.pth')

"""# **TODO6: Evaluation**

"""

import torch

# Assume we have already defined CharRNN and corresponding parameters
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
vocab_size = len(char_to_id)
embed_dim = 128
hidden_dim = 256

# Initialize the model and load the trained weights
model = CharRNN(vocab_size, embed_dim, hidden_dim).to(device)
model.load_state_dict(torch.load('char_rnn_model.pth'))

# Set the model to evaluation mode
model.eval()

def evaluate(eval_loader):
    correct_predictions = 0
    total_samples = 0
    sample_limit = 50  # Limit the number of samples to output

    with torch.no_grad():  # Disable gradient computation
        for x, y in eval_loader:
            # Move x and y to the correct device
            x, y = x.to(device), y.to(device)

            # Perform predictions using the model
            for i in range(x.size(0)):  # For each sample
                start_id = x[i].cpu().numpy().tolist()  # Get the list of input sequence IDs
                start_char = [id_to_char[id] for id in start_id]
                # print(start_char)
                predicted_str = model.generator(start_char)  # Generate characters using the generator

                true_ids = y[i].cpu().numpy()  # Directly get the true labels
                true_str = ''.join([id_to_char[id] for id in true_ids if id != char_to_id['<pad>'] and id != char_to_id['<eos>']])

                # Output the predicted and true strings for inspection (only for the first sample_limit samples)
                if total_samples < sample_limit:
                    print(f"Predicted: {predicted_str}, True: {true_str}")

                # Check if the predicted string matches the true string
                total_samples += 1
                if predicted_str == true_str:
                    correct_predictions += 1

    accuracy = correct_predictions / total_samples if total_samples > 0 else 0
    print(f"Accuracy (Exact Match): {accuracy:.4f}")

# Evaluate on the evaluation set
evaluate(eval_loader)

"""# **TODO4: Generation**

"""

model = CharRNN(vocab_size, embed_dim, hidden_dim).to(device)
model.load_state_dict(torch.load('char_rnn_model.pth'))

with torch.no_grad():
  user_str = input("Input your question: ")
  answer = model.generator(str(user_str))
  print(f"{user_str}{answer}")