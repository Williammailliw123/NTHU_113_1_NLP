# -*- coding: utf-8 -*-
"""NLP_HW3_NTHU_113062624.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HNg31dOo6XC8tQbnX-eMZ-r8Tq6eJ1-w
"""

!pip install datasets==2.21.0
!pip install torchmetrics

import transformers as T
from datasets import load_dataset
import torch
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from tqdm import tqdm
from torchmetrics import SpearmanCorrCoef, Accuracy, F1Score
device = "cuda" if torch.cuda.is_available() else "cpu"

# Some Chinese punctuation marks are encoded as [UNK] by the tokenizer,
# so replace them with English punctuation.
token_replacement = [
    ["：" , ":"],    # Replace Chinese colon with English colon
    ["，" , ","],    # Replace Chinese comma with English comma
    ["“" , "\""],    # Replace Chinese opening quotation mark with English double quote
    ["”" , "\""],    # Replace Chinese closing quotation mark with English double quote
    ["？" , "?"],    # Replace Chinese question mark with English question mark
    ["……" , "..."],  # Replace Chinese ellipsis with English ellipsis
    ["！" , "!"]     # Replace Chinese exclamation mark with English exclamation mark
]

class SemevalDataset(Dataset):
    def __init__(self, split="train") -> None:
        super().__init__()
        assert split in ["train", "validation", "test"]
        self.data = load_dataset(
            "sem_eval_2014_task_1", split=split, cache_dir="./cache/"
        ).to_list()

    def __getitem__(self, index):
        d = self.data[index]

        for k in ["premise", "hypothesis"]:
            for tok in token_replacement:
                d[k] = d[k].replace(tok[0], tok[1])
        return d

    def __len__(self):
        return len(self.data)

data_sample = SemevalDataset(split="train").data[:3]
print(f"Dataset example: \n{data_sample[0]} \n{data_sample[1]} \n{data_sample[2]}")

# Define the hyperparameters
lr = 1e-5
epochs = 5
train_batch_size = 8
validation_batch_size = 8
test_batch_size = 8

# TODO1: Create batched data for DataLoader
# Part of it is generated by ChatGPT and then modified.
from transformers import BertTokenizer
from torch.utils.data import DataLoader

# Initialize the tokenizer
tokenizer = T.BertTokenizer.from_pretrained("google-bert/bert-base-uncased", cache_dir="./cache/")

def collate_fn(batch):
    # TODO1-1: Implement the collate_fn function
    # This function takes a data batch (tuple) as input and packs it into tensors.
    # Use the tokenizer to tokenize and pack the data and its corresponding labels.
    # Return the data batch and labels for each sub-task.

    # Retrieve premise and hypothesis texts
    premises = [item["premise"] for item in batch]
    hypotheses = [item["hypothesis"] for item in batch]

    # Use the tokenizer to pack texts into tensors
    inputs = tokenizer(premises, hypotheses, padding=True, truncation=True, return_tensors="pt")

    # Pack relatedness_score and entailment_judgment into tensors
    relatedness_scores = torch.tensor([item["relatedness_score"] for item in batch], dtype=torch.float)
    entailment_judgments = torch.tensor([item["entailment_judgment"] for item in batch], dtype=torch.long)

    return inputs, relatedness_scores, entailment_judgments

# TODO1-2: Define your DataLoader
dl_train = DataLoader(
    SemevalDataset(split="train"),
    batch_size=train_batch_size,
    collate_fn=collate_fn,
    shuffle=True
)

dl_validation = DataLoader(
    SemevalDataset(split="validation"),
    batch_size=validation_batch_size,
    collate_fn=collate_fn,
    shuffle=False
)

dl_test = DataLoader(
    SemevalDataset(split="test"),
    batch_size=test_batch_size,
    collate_fn=collate_fn,
    shuffle=False
)

# TODO2: Construct your model
# Part of it is generated by ChatGPT and then modified.
import torch
from transformers import BertModel

class MultiLabelModel(torch.nn.Module):
    def __init__(self):
        super(MultiLabelModel, self).__init__()

        # Use BERT as the main encoder
        self.bert = BertModel.from_pretrained("bert-base-uncased")

        # Add linear layers for regression and classification
        self.regression_head = torch.nn.Linear(self.bert.config.hidden_size, 1)  # For relatedness_score
        self.classification_head = torch.nn.Linear(self.bert.config.hidden_size, 3)  # For entailment_judgment (assuming 3 classes)

    def forward(self, input_ids, attention_mask, token_type_ids):
        # Pass through the BERT model to get the hidden layer output (using [CLS] token representation)
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)

        # Extract the [CLS] representation
        cls_output = outputs.last_hidden_state[:, 0, :]  # [batch_size, hidden_size]

        # Outputs for regression and classification
        relatedness_score = self.regression_head(cls_output).squeeze(-1)  # [batch_size]
        entailment_judgment = self.classification_head(cls_output)  # [batch_size, num_classes]

        return relatedness_score, entailment_judgment

# TODO3: Define your optimizer and loss function
# Part of it is generated by ChatGPT and then modified.

from transformers import AdamW
from torch.optim import Adam
import torch.nn as nn

# Initialize the model and move it to the device
model = MultiLabelModel().to(device)

# TODO3-1: Define the optimizer
optimizer = Adam(model.parameters(), lr=lr)

# TODO3-2: Define the loss functions (you should have two)
regression_loss_fn = nn.MSELoss()  # For the regression task (relatedness_score)
classification_loss_fn = nn.CrossEntropyLoss()  # For the classification task (entailment_judgment)

# Define scoring functions
spc = SpearmanCorrCoef().to(device)
acc = Accuracy(task="multiclass", num_classes=3).to(device)
f1 = F1Score(task="multiclass", num_classes=3, average='macro').to(device)

# Part of it is generated by ChatGPT and then modified.
import torch
from tqdm import tqdm
from torchmetrics import SpearmanCorrCoef, Accuracy, F1Score

# Initialize evaluation metrics
spc_train = SpearmanCorrCoef().to(device)
acc_train = Accuracy(task="multiclass", num_classes=3).to(device)
f1_train = F1Score(task="multiclass", num_classes=3, average='macro').to(device)

for ep in range(epochs):
    # TODO4: Write the training loop
    # Training phase
    pbar = tqdm(dl_train)
    pbar.set_description(f"Training epoch [{ep+1}/{epochs}]")
    model.train()

    total_loss = 0
    for batch in pbar:
        # Clear gradients from the previous step
        optimizer.zero_grad()

        # Pack the data into tensors
        inputs, relatedness_scores, entailment_judgments = batch

        # Access input_ids, etc.
        input_ids = inputs['input_ids'].to(device)
        attention_mask = inputs['attention_mask'].to(device)
        token_type_ids = inputs['token_type_ids'].to(device)
        relatedness_scores = relatedness_scores.to(device)
        entailment_judgments = entailment_judgments.to(device)

        # Forward pass
        predicted_scores, predicted_classes = model(input_ids, attention_mask, token_type_ids)

        # Calculate loss
        loss_reg = regression_loss_fn(predicted_scores, relatedness_scores)
        loss_cls = classification_loss_fn(predicted_classes, entailment_judgments)
        loss = loss_reg + loss_cls  # Aggregate loss

        # Backward pass
        loss.backward()

        # Optimize the model
        optimizer.step()

        # Update total loss
        total_loss += loss.item()

        # Update progress bar display
        pbar.set_postfix(loss=total_loss / (pbar.n + 1))

        # Update training evaluation metrics
        spc_train.update(predicted_scores, relatedness_scores)
        acc_train.update(predicted_classes, entailment_judgments)
        f1_train.update(predicted_classes, entailment_judgments)

    # Calculate final training metrics
    train_spearman_corr = spc_train.compute()
    train_accuracy = acc_train.compute()
    train_f1_score = f1_train.compute()

    # Reset training metrics
    spc_train.reset()
    acc_train.reset()
    f1_train.reset()

    # Output training results
    print(f"Training Spearman Correlation: {train_spearman_corr}")
    print(f"Training Accuracy: {train_accuracy}")
    print(f"Training F1 Score: {train_f1_score}")

    # TODO5: Write the evaluation loop
    # Validation phase
    pbar = tqdm(dl_validation)
    pbar.set_description(f"Validation epoch [{ep+1}/{epochs}]")
    model.eval()

    with torch.no_grad():
        total_val_loss = 0
        for batch in pbar:
            # Pack the data into tensors
            inputs, relatedness_scores, entailment_judgments = batch

            # Access input_ids, etc.
            input_ids = inputs['input_ids'].to(device)
            attention_mask = inputs['attention_mask'].to(device)
            token_type_ids = inputs['token_type_ids'].to(device)
            relatedness_scores = relatedness_scores.to(device)
            entailment_judgments = entailment_judgments.to(device)

            # Forward pass
            predicted_scores, predicted_classes = model(input_ids, attention_mask, token_type_ids)

            # Calculate validation loss
            loss_reg = regression_loss_fn(predicted_scores, relatedness_scores)
            loss_cls = classification_loss_fn(predicted_classes, entailment_judgments)
            loss = loss_reg + loss_cls

            total_val_loss += loss.item()

            # Update evaluation metrics
            spc.update(predicted_scores, relatedness_scores)
            acc.update(predicted_classes, entailment_judgments)
            f1.update(predicted_classes, entailment_judgments)

        # Calculate final validation metrics
        spearman_corr = spc.compute()
        accuracy = acc.compute()
        f1_score = f1.compute()

        # Reset validation metrics
        spc.reset()
        acc.reset()
        f1.reset()

        # Output validation results
        print(f"Validation Spearman Correlation: {spearman_corr}")
        print(f"Validation Accuracy: {accuracy}")
        print(f"Validation F1 Score: {f1_score}")

    # Save model
    torch.save(model.state_dict(), f'./saved_models/ep{ep}.ckpt')

"""For test set predictions, you can write perform evaluation simlar to #TODO5."""

# Part of it is generated by ChatGPT and then modified.
# Testing phase
from torchmetrics import SpearmanCorrCoef, Accuracy, F1Score

# Initialize evaluation metrics for the test set
spc_test = SpearmanCorrCoef().to(device)
acc_test = Accuracy(task="multiclass", num_classes=3).to(device)
f1_test = F1Score(task="multiclass", num_classes=3, average='macro').to(device)

# Load the best model
model.eval()

with torch.no_grad():
    pbar = tqdm(dl_test)
    pbar.set_description("Testing")
    total_test_loss = 0

    for batch in pbar:
        # Pack the data into tensors
        inputs, relatedness_scores, entailment_judgments = batch

        # Access input_ids, etc.
        input_ids = inputs['input_ids'].to(device)
        attention_mask = inputs['attention_mask'].to(device)
        token_type_ids = inputs['token_type_ids'].to(device)
        relatedness_scores = relatedness_scores.to(device)
        entailment_judgments = entailment_judgments.to(device)

        # Forward pass
        predicted_scores, predicted_classes = model(input_ids, attention_mask, token_type_ids)

        # Calculate test loss
        loss_reg = regression_loss_fn(predicted_scores, relatedness_scores)
        loss_cls = classification_loss_fn(predicted_classes, entailment_judgments)
        loss = loss_reg + loss_cls
        total_test_loss += loss.item()

        # Update test evaluation metrics
        spc_test.update(predicted_scores, relatedness_scores)
        acc_test.update(predicted_classes, entailment_judgments)
        f1_test.update(predicted_classes, entailment_judgments)

    # Calculate final test metrics
    test_spearman_corr = spc_test.compute()
    test_accuracy = acc_test.compute()
    test_f1_score = f1_test.compute()

    # Reset test metrics
    spc_test.reset()
    acc_test.reset()
    f1_test.reset()

    # Output test results
    print(f"Test Spearman Correlation: {test_spearman_corr}")
    print(f"Test Accuracy: {test_accuracy}")
    print(f"Test F1 Score: {test_f1_score}")